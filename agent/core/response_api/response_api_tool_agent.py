# --------------------------------openaiçš„harmony responseæ¥å£çš„ä¸»è¦ç‰¹æ€§--------------------------------
# 1ã€response.create()ä¸­ï¼Œinputå¯¹åº”completions.createä¸­çš„messagesï¼Œä¸”å¿…é¡»ä¸ºä¸‹è¿°æ ¼å¼ï¼š
#       {'role': 'user', 'content': 'ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯åœŸåœŸ'}
#       {'role': 'assistant', 'content': 'ä½ å¥½ï¼ŒåœŸåœŸï¼è¯·å‘Šè¯‰æˆ‘éœ€è¦æˆ‘å¸®ä½ å®Œæˆä»€ä¹ˆä»»åŠ¡ã€‚ç¥ä½ æ„‰å¿«ï¼'}
#   æ³¨æ„ä¸‹è¿°"type": "message"èµ·å§‹çš„æ ¼å¼æ˜¯é”™è¯¯çš„ï¼Œllmæ— æ³•æŠŠå…¶å½“ä½œæœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œä½†åˆä¸æŠ¥é”™
#       {
#         "type": "message",
#         "role": "user",
#         "content": [{"type": "input_text", "text": 'ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯åœŸåœŸ'}],
#       }
# 2ã€response.create()ä¸­ï¼Œinstructionså¯¹åº”completions.createä¸­çš„system prompt
# 3ã€response.create()ä¸­ï¼Œè°ƒç”¨å·¥å…·åï¼Œç›´æ¥input += res.outputï¼Œä¸”input.appendä¸‹è¿°ç‰¹å®šæ ¼å¼çš„tool resultåï¼Œå¹¶å°†å…¶ä½œä¸ºæ–°çš„inputå†æ¬¡è°ƒç”¨response.create()å³å¯è¿›è¡Œè¿ç»­å·¥å…·è°ƒç”¨
#         {
#             "type": "function_call_output",
#             "call_id": "call_456",
#             "output": "Weather(city='Tokyo', temperature_range='14-20C', conditions='Sunny')"
#         }
# 4ã€response.create()ä¸­ï¼Œåœ¨resè¾“å‡ºoutput(assistantçš„textè¾“å‡ºï¼‰åï¼Œéœ€è¦ä¸€ä¸ªæ–°çš„{'role': 'user', 'content': ...}å¼€å¯ä¸‹ä¸€è½®å¯¹è¯æˆ–å·¥å…·è°ƒç”¨ä»»åŠ¡
# 5ã€response.create()è¿ç»­è°ƒç”¨å·¥å…·å®Œæˆä»»åŠ¡åä¼šè¾“å‡ºoutputï¼Œæ­¤æ—¶è‹¥ç»§ç»­è°ƒç”¨response.create()è¿›è¡Œæ–°ä¸€è½®runï¼Œä¼¼ä¹å¿…é¡»å°†inputå†å²ä¸­çš„ResponseReasoningItemã€ResponseFunctionToolCallã€ResponseOutputMessageç­‰æ¸…é™¤ï¼Œä¸ç„¶ä¼šæŠ¥é”™

import config
import llm_protocol
from llm_protocol import LLM_Config
from agent.tools.protocol import Tool_Call_Paras
from tools.llm.response_api_client import Response_LLM_Client, Response_Result, Tool_Request, Tool_Parameters, Tool_Property, Response_Request

from agent.core.agent_config import Agent_Config
from agent.core.protocol import Query_Agent_Context, Agent_Status
from agent.tools.protocol import Tool_Call_Paras, Action_Result

from config import dred, dgreen, dblue, dyellow, dcyan
from pprint import pprint
from uuid import uuid4
import json

from console import err, agent_query_output, agent_tool_chosen_output, agent_tool_result_output, agent_finished_output

DEBUG = config.Global.app_debug

def dprint(*args, **kwargs):
    if DEBUG:
        print(*args, **kwargs)

def dpprint(*args, **kwargs):
    if DEBUG:
        pprint(*args, **kwargs)

class Response_API_Tool_Agent:
    def __init__(self,
                 agent_config:Agent_Config,
                 agent_max_retry=config.Agent.MAX_RETRY,
                 agent_max_error_retry=config.Agent.MAX_ERROR_RETRY
                 ):
        self.llm_config = agent_config.llm_config
        self.response_llm_client = Response_LLM_Client(self.llm_config)
        self.agent_config = agent_config

        # è‡ªå·±çš„id
        self.agent_id = str(uuid4())
        # å¤šå±‚agentä½“ç³»ä¸­çš„é¡¶å±‚agentçš„id
        self.top_agent_id = self.agent_config.top_agent_id if self.agent_config.top_agent_id else self.agent_id

        # ç»éªŒ
        self.current_exp_str = ''

        # agentæœ€å¤§å‡ºé”™æ¬¡æ•°
        self.agent_max_error_retry = agent_max_error_retry
        # agentæœ€å¤§æ¬¡æ•°
        self.agent_max_retry = agent_max_retry

        self.agent_status = Agent_Status()

        # self.final_answer_flag = 'ã€ä»»åŠ¡å®Œæˆã€‘'
        # self.decide_final_answer_prompt = f'å½“å®Œæˆä»»åŠ¡æ—¶ï¼Œè¯·è¾“å‡º"{self.final_answer_flag}"ï¼Œå¦åˆ™ç³»ç»Ÿæ— æ³•åˆ¤æ–­ä½•æ—¶ç»“æŸä»»åŠ¡ã€‚'


    def init(self):
        self.response_llm_client.init()

    def _call_tool(self,
                   response_result:Response_Result, # response_apiçš„è°ƒç”¨ç»“æœ
                   tool_call_paras:Tool_Call_Paras, # agentè°ƒåº¦çš„ä¸Šä¸‹æ–‡
                   ):
        tool_call = response_result.function_tool_call
        if tool_call and 'name' in tool_call:
            tool_name = tool_call['name']

            for func in self.response_llm_client.funcs:
                if func['name'] in tool_name:   # vllmçš„response apiæœ‰æ—¶å€™ä¼šå‡ºé”™ï¼Œå¦‚ï¼š'name': 'div_tool<|channel|>json' è€Œä¸æ˜¯ 'name': 'div_tool'
                # if tool_name == func['name']:
                    try:
                        agent_tool_chosen_output(tool_name=tool_name, tool_paras=tool_call['arguments'])
                        args = json.loads(tool_call['arguments'])

                        # -----------------------------å·¥å…·è°ƒç”¨-----------------------------
                        # tool_call_paras.callback_tool_paras_dict = args
                        func_rtn = func['func'](tool_call_paras=tool_call_paras, **args)
                        # ----------------------------/å·¥å…·è°ƒç”¨-----------------------------

                        dprint('-----------------------------å·¥å…·è°ƒç”¨ç»“æœ-------------------------------')
                        dprint(f'tool_name = "{tool_name}"')
                        dprint(func_rtn)
                        dprint('----------------------------/å·¥å…·è°ƒç”¨ç»“æœ-------------------------------')
                        if isinstance(func_rtn, Action_Result):
                            response_result.tool_call_result = json.dumps(func_rtn.model_dump(), ensure_ascii=False)
                        else:
                            response_result.tool_call_result = json.dumps(func_rtn, ensure_ascii=False)

                        # tool_call_result_item = {
                        #     "type": "function_call_output",
                        #     "call_id": tool_call['call_id'],
                        #     "output": json.dumps({tool_call['name']: response_result.tool_call_result}),
                        #     "error": response_result.error
                        # }

                        self.response_llm_client.history_input_add_tool_call_result_item(
                            arguments=tool_call['arguments'],
                            call_id=tool_call['call_id'],
                            output=json.dumps({tool_call['name']: response_result.tool_call_result}, ensure_ascii=False),
                            error=response_result.error
                        )
                        agent_tool_result_output(json.loads(response_result.tool_call_result).get('result'))
                        # self.response_llm_client.history_input_list.append(tool_call_result_item)

                        return response_result
                    except Exception as e:
                        err(e)
                        response_result.error = e
                        # response_result.tool_call_result = e
                        dred(f'ã€Response_API_Tool_Agent._call_tool()ã€‘responses_result.error: {e!r}')
                        agent_tool_result_output(response_result.error)
                        return response_result

        return response_result

    def _run_before(self, query):
        agent_query_output(query)

    def _run_after(self):
        self.agent_status.finished_one_run = True

        # --------------------------------
        # ä¸€è½®runç»“æŸåï¼Œéœ€è¦å°†input_listä¸­çš„ResponseReasoningItemã€ResponseFunctionToolCallå’ŒResponseOutputMessageæ¸…é™¤
        # å¦åˆ™serverä¼šæŠ¥validation errors for ValidatorIteratorçš„é”™è¯¯
        self.response_llm_client.history_input_reduce_content_after_this_run()

        dgreen('-----------------------------------æœ€ç»ˆç»“æœ---------------------------------------------')
        dgreen(self.agent_status.final_answer)
        dgreen('-----------------------------------æœ€ç»ˆç»“æœ---------------------------------------------')
        # print(f'final: {self.agent_status.final_answer}')
        agent_finished_output(self.agent_status.final_answer)

    def run(self, query, tools):
        self._run_before(query)

        use_chatml = self.response_llm_client.llm_config.chatml

        # query_with_final_answer_flag = query + '\n' + self.decide_final_answer_prompt
        # dblue(f'-------------------------------query_with_final_answer_flag-------------------------------')
        # dblue(query_with_final_answer_flag)
        # dblue(f'------------------------------/query_with_final_answer_flag-------------------------------')

        agent_err_count = 0
        agent_count = 0

        response_request = Response_Request(
            # instructions=query_with_final_answer_flag,  # è¿™é‡Œä»ç„¶æ˜¯'è¯·å‘Šè¯‰æˆ‘2356/3567+22*33+3567/8769+4356/5678ç­‰äºå¤šå°‘ï¼Œä¿ç•™10ä½å°æ•°ï¼Œè¦è°ƒç”¨å·¥å…·è®¡ç®—ï¼Œä¸èƒ½ç›´æ¥å¿ƒç®—'
            # input=query_with_final_answer_flag,       # ç¬¬ä¸€æ¬¡è¯·æ±‚inputç”¨queryï¼Œç¬¬äºŒæ¬¡åŠä»¥åçš„è¯·æ±‚ï¼Œinputå®é™…ç”¨äº†self.history_input_list
            # instructions='ç»§ç»­è°ƒç”¨å·¥å…·ç›´åˆ°å®Œæˆuserçš„ä»»åŠ¡',
            model=self.llm_config.llm_model_id,
            tools=tools,
            temperature=self.llm_config.temperature,
            top_p=self.llm_config.top_p,
            max_output_tokens=self.llm_config.max_new_tokens,
            reasoning={"effort": self.llm_config.reasoning_effort},
        )
        responses_result = Response_Result()

        while not hasattr(responses_result, 'output') or responses_result.output=='' or responses_result.output is None:
            agent_count += 1
            if agent_err_count >= self.agent_max_error_retry:
                dred(f'ã€Response_API_Tool_Agent.run()ã€‘å‡ºé”™æ¬¡æ•°è¶…å‡ºagent_max_error_retry({self.agent_max_error_retry})ï¼Œé€€å‡ºå¾ªç¯.')
                break

            if agent_count >= self.agent_max_retry:
                dred(f'ã€Response_API_Tool_Agent.run()ã€‘è°ƒç”¨æ¬¡æ•°è¶…å‡ºagent_max_retry({self.agent_max_retry})ï¼Œé€€å‡ºå¾ªç¯.')
                break

            try:
                # query = query_with_final_answer_flag

                new_run = False
                if self.agent_status.finished_one_run:
                    new_run = True
                    self.agent_status.finished_one_run = False

                # dred(tools)
                if use_chatml:
                    responses_result = self.response_llm_client.chatml_create(query=query, request=response_request, new_run=new_run)
                else:
                    responses_result = self.response_llm_client.responses_create(query=query, request=response_request, new_run=new_run)
                # dpprint(responses_result.model_dump())
            except Exception as e:
                err(e)
                agent_err_count += 1
                responses_result.error = e
                continue

            # tool_call_paras = None

            # æœ‰æ—¶å€™æ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œç›´æ¥output
            if not responses_result.function_tool_call:
                if responses_result.output:
                    self.agent_status.final_answer = responses_result.output.strip()
                else:
                    self.agent_status.final_answer = responses_result.output

                self.response_llm_client.history_input_add_output_item(self.agent_status.final_answer)
                self._run_after()
                return self.agent_status
            # if not responses_result.function_tool_call:
            #     # dred(f'function_tool_callä¸ºç©º2, responses_result.output:{responses_result.output!r}')
            #     if self.final_answer_flag in responses_result.output:
            #         self.agent_status.final_answer = responses_result.output.replace(self.final_answer_flag, '').strip()
            #
            #         self.response_llm_client.history_input_add_output_item(self.agent_status.final_answer)
            #         self._run_after()
            #         return self.agent_status
            #     else:
            #         continue
            if use_chatml:
                tool_params_dict = json.loads(responses_result.function_tool_call['arguments']) if responses_result.function_tool_call['arguments'] else None
            else:
                tool_params_dict = json.loads(responses_result.function_tool_call['arguments'])
            tool_call_paras = Tool_Call_Paras(
                callback_top_agent_id=self.top_agent_id,
                callback_tool_paras_dict=tool_params_dict,
                callback_agent_config=self.agent_config,
                callback_agent_id=self.agent_id,
                # callback_last_tool_ctx=last_tool_ctx,
                # callback_client_ctx=context,
                callback_father_agent_exp=self.current_exp_str
            )

            dyellow('-----------------------------tool_call_paras----------------------------------')
            for item in tool_call_paras:
                dyellow(item)
            dyellow('----------------------------/tool_call_paras----------------------------------')
            responses_result = self._call_tool(responses_result, tool_call_paras)
            # responses_result = self.response_llm_client.legacy_call_tool(responses_result)

            if responses_result is None:
                continue

        self.agent_status.final_answer = responses_result.output.strip()

        self.response_llm_client.history_input_add_output_item(self.agent_status.final_answer)
        self._run_after()
        return self.agent_status

def main_response_agent():
    add_tool = Tool_Request(
        name='add_tool',
        description='åŠ æ³•è®¡ç®—å·¥å…·',
        parameters=Tool_Parameters(
            properties={
                'a': Tool_Property(type='number', description='å‚æ•°1'),
                'b': Tool_Property(type='number', description='å‚æ•°2'),
                # 'unit': Tool_Property(type='string', description='å•ä½', enum=['meter', 'kilo-miter']),
            },
            required=['a', 'b'],
        ),
        func=lambda a, b, **kwargs: {"result": a + b}
        # func=lambda a, b, unit: {"result": a + b, "unit": unit}
    )
    sub_tool = Tool_Request(
        name='sub_tool',
        description='å‡æ³•è®¡ç®—å·¥å…·',
        parameters=Tool_Parameters(
            properties={
                'a': Tool_Property(type='number', description='å‚æ•°1'),
                'b': Tool_Property(type='number', description='å‚æ•°2'),
                # 'unit': Tool_Property(type='string', description='å•ä½', enum=['meter', 'kilo-miter']),
            },
            required=['a', 'b'],
        ),
        func=lambda a, b, **kwargs: {"result": a - b}
        # func=lambda a, b, unit: {"result": a - b, "unit": unit}
    )
    mul_tool = Tool_Request(
        name='mul_tool',
        description='ä¹˜æ³•è®¡ç®—å·¥å…·',
        parameters=Tool_Parameters(
            properties={
                'a': Tool_Property(type='number', description='å‚æ•°1'),
                'b': Tool_Property(type='number', description='å‚æ•°2'),
                # 'unit': Tool_Property(type='string', description='å•ä½', enum=['meter', 'kilo-miter']),
            },
            required=['a', 'b'],
        ),
        func=lambda a, b, **kwargs: {"result": a * b}
        # func=lambda a, b, unit: {"result": a * b, "unit": unit}
    )
    div_tool = Tool_Request(
        name='div_tool',
        description='é™¤æ³•è®¡ç®—å·¥å…·',
        parameters=Tool_Parameters(
            properties={
                'a': Tool_Property(type='number', description='å‚æ•°1'),
                'b': Tool_Property(type='number', description='å‚æ•°2'),
                # 'unit': Tool_Property(type='string', description='å•ä½', enum=['meter', 'kilo-miter']),
            },
            required=['a', 'b'],
        ),
        func=lambda a, b, **kwargs: {"result": a / b}
        # func=lambda a, b, unit: {"result": a / b, "unit": unit}
    )

    from agent.tools.folder_tool import Folder_Tool
    fold_tool = Folder_Tool.get_tool_param_dict()

    tools = [fold_tool, add_tool, sub_tool, mul_tool, div_tool]

    agent_config = Agent_Config(
        agent_name = 'agent for search folder',
        tool_names=['Folder_Tool'],
        # llm_config=llm_protocol.g_local_qwen3_30b_thinking,
        # llm_config=llm_protocol.g_local_qwen3_30b_chat,
        llm_config=llm_protocol.g_online_groq_gpt_oss_20b,
        # llm_config=llm_protocol.g_online_groq_gpt_oss_120b,
        # llm_config=llm_protocol.g_local_gpt_oss_20b_mxfp4,
        # llm_config=llm_protocol.g_local_gpt_oss_20b_mxfp4_lmstudio,
        # llm_config=llm_protocol.g_local_gpt_oss_120b_mxfp4_lmstudio,
        has_history=True,
    )

    # query = 'è¯·å‘Šè¯‰æˆ‘/home/tutu/demoä¸‹çš„å“ªä¸ªå­ç›®å½•é‡Œæœ‰file_to_find.txtè¿™ä¸ªæ–‡ä»¶ï¼Œé€’å½’æœç´¢æ‰€æœ‰å­æ–‡ä»¶å¤¹ç›´åˆ°å‡†ç¡®æ‰¾åˆ°è¯¥æ–‡ä»¶'
    query = 'è¯·å‘Šè¯‰æˆ‘2356/3567+22*33+3567/8769+4356/5678ç­‰äºå¤šå°‘ï¼Œä¿ç•™10ä½å°æ•°ï¼Œè¦è°ƒç”¨å·¥å…·è®¡ç®—ï¼Œä¸èƒ½ç›´æ¥å¿ƒç®—'
    # query = 'ä½ æ˜¯è°ï¼Ÿ'

    agent = Response_API_Tool_Agent(agent_config=agent_config)
    agent.init()
    # agent.run(query=query, tools=tools)

    # agent.run(query='ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯åœŸåœŸ', tools=tools)
    agent.run(query=query, tools=tools)
    # agent.run(query='ä½ è¿˜è®°å¾—æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆå—ï¼Ÿè¿˜æœ‰ä¹‹å‰ä½ å·²ç»æ‰¾åˆ°äº†file_to_find.txtï¼Œå‘Šè¯‰æˆ‘å…·ä½“æ˜¯åœ¨å“ªé‡Œæ‰¾åˆ°', tools=tools)

    # agent.run(query='ä½ å¥½ï¼Œæˆ‘çš„åå­—æ˜¯åœŸåœŸ', tools=tools)
    # agent.run(query='ä½ è¿˜è®°å¾—æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆå—ï¼Ÿ', tools=tools)

def main_response_agent_mcp_stdio():
    from openai import OpenAI
    import httpx
    import llm_protocol
    import config

    http_client = httpx.Client(proxy=config.g_vpn_proxy)
    llm_config = llm_protocol.g_online_groq_gpt_oss_20b

    client = OpenAI(
        api_key=llm_config.api_key,
        base_url=llm_config.base_url,
        http_client=http_client,
    )

    resp = client.responses.create(
        model=llm_config.llm_model_id,
        tools=[
            {
                "type": "mcp",
                "server_label": "everything",
                "server_description": "Local MCP server (server-everything via STDIO)",
                "server_command": "npx",
                "server_args": ["@modelcontextprotocol/server-everything"],
                "server_transport": "stdio",  # ğŸ‘ˆ å…³é”®æ”¹è¿™é‡Œ
                "require_approval": "never",
            },
        ],
        input="ä½ æ€ä¹ˆç”¨",
    )

    print(resp.output_text)

def main_response_agent_mcp_server():
    from openai import OpenAI
    import httpx
    import llm_protocol

    http_client = httpx.Client(proxy=config.g_vpn_proxy)
    llm_config = llm_protocol.g_online_groq_gpt_oss_20b
    client = OpenAI(
        api_key=llm_config.api_key,
        base_url=llm_config.base_url,
        http_client=http_client,
    )

    resp = client.responses.create(
        model=llm_config.llm_model_id,
        tools=[
            {
                "type": "mcp",
                "server_label": "dmcp",
                "server_description": "A Dungeons and Dragons MCP server to assist with dice rolling.",
                "server_url": "https://dmcp-server.deno.dev/sse",
                "require_approval": "never",
            },
        ],
        input="Roll 2d4+1",
    )

    print(resp.output_text)

if __name__ == "__main__":
    main_response_agent()
    # main_response_agent_mcp_server()
    # main_response_agent_mcp_stdio()
