import gradio as gr

from tools.llm.api_client_qwen_openai import *
from tools.doc.llm_doc import *
from tools.retriever.search import search

llm = LLM_Qwen(
    need_print=False,
    # temperature=0,
)
# llm.set_role_prompt('‰Ω†Ê≠£Âú®ÊâÆÊºî‰∏Ä‰∏™Â•≥Â≠©Ôºå‰Ω†Â•ΩÁ¨®Á¨®„ÄÇ')

# ============================ÂÖ≥‰∫éËßíËâ≤ÊèêÁ§∫============================
# ‰∏Ä„ÄÅ‰Ω†Â∏åÊúõllm‰∫ÜËß£Âì™‰∫õ‰ø°ÊÅØÔºö
# 1) where are you based?
# 2) what do you do for work?
# 3) what are your hobbies and interests?
# 4) what subject can you talk about for hours?
# 5) what are some goals you have?
# ‰∫å„ÄÅ‰Ω†Â∏åÊúõllmÊÄéÊ†∑ÂõûÂ§çÔºö
# 1) how formal or casual should llm be?
# 2) how long or short should responses generally be?
# 3) how do you want to be addressed?
# 4) should llm have opinions on topics or remain neutral?
# ============================ÂÖ≥‰∫éËßíËâ≤ÊèêÁ§∫============================

class Shared():
    internet = False

    @staticmethod
    def set_internet(flag):
        Shared.internet = flag
        print(f'Shared.internet: {Shared.internet}')

def llm_async_ask(message, history):
    # gradioÁöÑÂÖ∏ÂûãÂØπËØùÊ†ºÂºèÔºö [['ÊàëÂè´ÂúüÂúü', '‰Ω†Â•ΩÔºåÂúüÂúüÔºÅÂæàÈ´òÂÖ¥ËÆ§ËØÜ‰Ω†„ÄÇ'], [], []]

    for item in llm.ask_prepare(message).get_answer_generator():
        yield item
    llm.print_history()

def llm_undo():
    print('ÊâßË°ållm_undo()')
    llm.undo()
    llm.print_history()

def llm_retry(history, message):
    print('ÊâßË°ållm_retry()')
    if history is not None and len(history)>0:
        message = history[-1][0]
        history[-1][1] = ""
        for chunk in llm.get_retry_generator():
            history[-1][1] += chunk
            yield history, message
    llm.print_history()

def llm_clear():
    print('ÊâßË°ållm_clear()')
    llm.clear_history()
    llm.print_history()

def llm_answer(history, message):
    print('---------------------ÊâßË°ållm_answer()---------------------')
    message = history[-1][0]
    if '‰∏ä‰º†Êñá‰ª∂' in history[-1][0]:
        filename = history[-1][0][0]
        # if 'docx' in filename:
        #     print(f'‰∏ä‰º†‰∫ÜÊñá‰ª∂: {history[-1][0][0]}')
        #     history[-1][1] = "" # history[-1][1]Âç≥‰∏∫botÁöÑËæìÂá∫
        #     doc = LLM_Doc(filename)
        #     doc.parse_all_docx()
        #     toc = doc.get_toc_md_string()
        #     history[-1][1] += toc
        #     print(toc)
        #     yield history, ''
    else:
        if current_file != '' and 'docx' in current_file:
            # Â∑≤ÊúâdocxÊñá‰ª∂‰∏ä‰º†
            doc = LLM_Doc(current_file)
            doc.llm.need_print = False
            doc.parse_all_docx()
            toc = doc.get_toc_md_for_tool_by_node(doc.doc_root)
            print(f'-----------------------toc----------------------------------')
            print(f'{toc}')
            # tables = doc.get_all_tables()
            tool = doc.llm_classify_question(message)
            answer_gen = doc.call_tools(tool, message, toc, in_tables=None)
            print('user: ', message)
            history[-1][1] = ""
            print('assistant: ', end='')
            for chunk in answer_gen:
                history[-1][1] += chunk
                print(chunk, end='', flush=True)
                yield history, message
            print()
        elif current_file != '' and 'pdf' in current_file:
            # Â∑≤ÊúâpdfÊñá‰ª∂‰∏ä‰º†
            doc = LLM_Doc(current_file)
            doc.llm.need_print = False
            doc.parse_all_pdf()
            toc = doc.get_toc_md_for_tool_by_node(doc.doc_root)
            print(f'-----------------------toc----------------------------------')
            print(f'{toc}')
            # tables = doc.get_all_tables()
            tool = doc.llm_classify_question(message)
            answer_gen = doc.call_tools(tool, message, toc)
            # answer_gen = doc.call_tools(tool, message, toc, tables)
            print('user: ', message)
            history[-1][1] = ""
            print('assistant: ', end='')
            for chunk in answer_gen:
                history[-1][1] += chunk
                print(chunk, end='', flush=True)
                yield history, message
            print()
        elif Shared.internet==False:
            print('user: ',message)
            history[-1][1] = ""
            print('assistant: ', end='')
            for chunk in llm_async_ask(message, history):
                history[-1][1] += chunk
                print(chunk, end='', flush=True)
                yield history, message
            print()
        elif Shared.internet==True:
            print('user: ',message)
            history[-1][1] = ""
            print('assistant: ', end='')

            results = search(message)
            url_idx = 0
            for url, content_para_list in results:
                # -------------ÁïåÈù¢‰∏äÁîüÊàê‰∏Ä‰∏™urlÂØπÂ∫îÁöÑÂÜÖÂÆπ-------------
                content = " ".join(content_para_list)
                if len(content) < 5000:
                    url_idx += 1

                    prompt = f'ËøôÊòØÁΩëÁªúÊêúÁ¥¢ÁªìÊûú: "{content}", ËØ∑Ê†πÊçÆËØ•ÊêúÁ¥¢ÁªìÊûúÁî®‰∏≠ÊñáÂõûÁ≠îÁî®Êà∑ÁöÑÊèêÈóÆ: "{message}"ÔºåÂõûÂ§çË¶ÅÁÆÄÊòéÊâºË¶Å„ÄÅÂ±ÇÊ¨°Ê∏ÖÊô∞„ÄÅÈááÁî®markdownÊ†ºÂºè„ÄÇ'
                    temp_llm = LLM_Qwen(history=False)
                    gen = temp_llm.ask_prepare(prompt).get_answer_generator()

                    for chunk in gen:
                        history[-1][1] += chunk     # ËæìÂá∫llmÁªìÊûú
                        print(chunk, end='', flush=True)
                        yield history, message
                    # history[-1][1] += f'\n## &nbsp; '  # ËæìÂá∫Á©∫Ë°å
                    # yield history, message

                    print(f'\n\nÂá∫Â§Ñ[{url_idx}]: ' + url + '\n\n')
                    history[-1][1] += f'\n#### Âá∫Â§Ñ[{url_idx}]: ' + url + '\n## &nbsp; \n## &nbsp; ' # ËæìÂá∫url
                    yield history, message

def bot_add_text(history, text, role_prompt):
    llm.set_role_prompt(role_prompt)
    history = history + [(text, None)]
    print('bot add text: ',text)
    print("bot's history: ", history)
    return history, gr.update(value="", interactive=False)

def bot_undo(history):
    if history is not None and len(history)>0:
        history.pop()
    print('history: ', history)
    return history, gr.update(value="", interactive=False)

def bot_retry(history, role_prompt):
    llm.set_role_prompt(role_prompt)
    if len(history)>0:
        history[-1][1]=''
    return history, gr.update(value="", interactive=False)

def bot_clear(history):
    if history is not None:
        history.clear()
    print('history: ', history)
    return history, gr.update(value="", interactive=False)

current_file = ''
def bot_on_upload(history, file):
    global current_file
    current_file = file.name
    history = history + [((file.name,'‰∏ä‰º†Êñá‰ª∂'), None)]
    print('bot_on_upload, history: ', history)
    return history

# def llm_on_role_prompt_change(prompt):
#     llm.set_role_prompt(prompt)

from typing import Iterable
from gradio.themes.utils import colors, fonts, sizes
from gradio.themes.default import Default
class Qwen_Theme(Default):
    def __init__(
        self,
        *,
        primary_hue: colors.Color | str = colors.emerald,
        secondary_hue: colors.Color | str = colors.blue,
        neutral_hue: colors.Color | str = colors.gray,
        spacing_size: sizes.Size | str = sizes.spacing_sm,
        radius_size: sizes.Size | str = sizes.radius_sm,
        text_size: sizes.Size | str = sizes.text_sm,
        font: fonts.Font
        | str
        | Iterable[fonts.Font | str] = (
            fonts.GoogleFont("Quicksand"),
            "ui-sans-serif",
            "sans-serif",
        ),
        font_mono: fonts.Font
        | str
        | Iterable[fonts.Font | str] = (
            fonts.GoogleFont("IBM Plex Mono"),
            "ui-monospace",
            "monospace",
        ),
    ):
        super().__init__(
            primary_hue=primary_hue,
            secondary_hue=secondary_hue,
            neutral_hue=neutral_hue,
            spacing_size=spacing_size,
            radius_size=radius_size,
            text_size=text_size,
            font=font,
            font_mono=font_mono,
        )

qwen_theme = Qwen_Theme()
def main():
    # gr.themes.builder()
    # pass
    with gr.Blocks(theme=qwen_theme) as demo:
        chatbot = gr.Chatbot(
            [],
            elem_id="chatbot",
            # layout='panel',
            # layout='bubble',
            show_copy_button=True,
            # line_breaks = False,
            # render_markdown=False,
            bubble_full_width=False,
            height=500,
            # avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
        )

        with gr.Row():
            user_input = gr.Textbox(
                lines=5,
                max_lines=20,
                autofocus=True,
                scale=32,
                show_label=False,
                placeholder="ËæìÂÖ•ÊñáÊú¨Âπ∂ÊåâÂõûËΩ¶ÔºåÊàñËÄÖ‰∏ä‰º†Êñá‰ª∂",
                container=False,
            )
            with gr.Column(scale=1, min_width=80):
                internet_cbx = gr.Checkbox(value=False, label="ËÅîÁΩë", min_width=80)
            submit_btn = gr.Button(value="Êèê‰∫§", min_width=50, scale=1)


        with gr.Row():
            dark_mode_btn = gr.Button(
                "üí°",
                scale=1,
                size="sm",
                min_width=20,
                variant="primary",
            )
            upload_btn = gr.UploadButton(
                "üìÅ",
                scale=1,
                size='sm',
                min_width=20,
                file_types=["image", "text", "video", "audio"],
                # file_count = "multiple"
            )
            undo_btn = gr.Button(value="ÂõûÊí§", min_width=20,scale=3)
            retry_btn = gr.Button(value="ÈáçËØï",min_width=20, scale=3)
            clear_btn = gr.Button(value="Ê∏ÖÁ©∫", min_width=20,scale=6)


        with gr.Accordion("È´òÁ∫ßËÆæÁΩÆ", open=False):
        # with gr.Row():
            role_prompt_tbx = gr.Textbox(
                value='',
                lines=10,
                max_lines=20,
                # scale=16,
                show_label=False,
                placeholder="ËæìÂÖ•ËßíËâ≤ÊèêÁ§∫ËØ≠",
                container=False,
            )
        # role_prompt.change(llm_on_role_prompt_change, role_prompt, None)

        internet_cbx.change(
            fn=Shared.set_internet,
            inputs=internet_cbx,
        )

        undo_btn.click(
            bot_undo,
            [chatbot],
            [chatbot, user_input],
            queue=False
        ).then(
            llm_undo,
            None,
            None
        ).then(
            lambda: gr.update(interactive=True),
            None,
            [user_input],
            queue=False
        )

        retry_btn.click(
            bot_retry,
            [chatbot, role_prompt_tbx],
            [chatbot, user_input],
            queue=False
        ).then(
            llm_retry,
            [chatbot, user_input],
            [chatbot, user_input]
        ).then(
            lambda: gr.update(interactive=True),
            None,
            [user_input],
            queue=False
        )

        clear_btn.click(
            bot_clear,
            [chatbot],
            [chatbot, user_input],
            queue=False
        ).then(
            llm_clear,
            None,
            None
        ).then(
            lambda: gr.update(interactive=True),
            None,
            [user_input],
            queue=False
        )

        submit_btn.click(
            bot_add_text,
            [chatbot, user_input, role_prompt_tbx],
            [chatbot, user_input],
            queue=False
        ).then(
            llm_answer,
            [chatbot, user_input],
            [chatbot, user_input],
        ).then(
            lambda: gr.update(interactive=True),
            None,
            [user_input],
            queue=False
        )

        txt_msg = user_input.submit(
            bot_add_text,
            [chatbot, user_input, role_prompt_tbx],
            [chatbot, user_input],
            queue=False
        ).then(
            llm_answer,
            [chatbot, user_input],
            [chatbot, user_input]
        ).then(
            lambda: gr.update(interactive=True),
            None,
            [user_input],
            queue=False
        )
        file_msg = upload_btn.upload(bot_on_upload, [chatbot, upload_btn], [chatbot], queue=False)
        # file_msg = upload_btn.upload(bot_on_upload, [chatbot, upload_btn], [chatbot], queue=False).then(
        #     llm_answer, [chatbot, user_input], [chatbot, user_input]
        # )

        dark_mode_btn.click(
            None,
            None,
            None,
            _js="""() => {
            if (document.querySelectorAll('.dark').length) {
                document.querySelectorAll('.dark').forEach(el => el.classList.remove('dark'));
            } else {
                document.querySelector('body').classList.add('dark');
            }
        }""",
            api_name=False,
        )

    demo.queue().launch()

if __name__ == "__main__":
    main()
